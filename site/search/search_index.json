{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Chained Legal Reasoning Benchmark for Large Language Models The Problem Existing legal AI benchmarks evaluate models on independent, parallel tasks . A model answers a citation question, gets scored, then answers an unrelated extraction question, gets scored again. Each task exists in isolation. But legal reasoning doesn't work that way. A lawyer researching case law performs sequential, dependent operations : find relevant authority \u2192 verify it's still good law \u2192 extract the holding \u2192 apply it to new facts \u2192 synthesize into analysis. Each step builds on the last. An error at step 2 corrupts everything downstream. L10 Agentic is the first U.S. common law benchmark that evaluates LLMs on chained legal reasoning \u2014measuring not just whether models get individual answers right, but whether they can maintain coherent reasoning across a complete legal research workflow. The Chain RULE PHASE S1 Known Authority S2 Unknown Authority S3 Validate Authority \u2193 APPLICATION PHASE S4 Fact Extraction S5 Distinguish Cases ( cb + rag variants ) \u2193 CONCLUSION PHASE S6 IRAC Synthesis S7 Citation Integrity \u2190 gate : fabrication voids S6 Step Definitions Step Name Task Ground Truth S1 Known Authority Retrieve citation and metadata for a given case cited_case.us_cite , term S2 Unknown Authority Find cases that cite the anchor case edge.citing_case_us_cite (MRR) S3 Validate Authority Determine if case has been overruled instance.overrule S4 Fact Extraction Extract disposition and winning party SCDB codes (closed enum) S5:cb Distinguish Cases Determine doctrinal relationship (metadata + holding) edge.agree S5:rag Distinguish Cases Determine doctrinal relationship (full opinion texts) edge.agree S6 IRAC Synthesis Generate structured legal analysis Deterministic rubric S7 Citation Integrity Verify no fabricated citations fake_cases + scdb A model achieving 90% accuracy per step completes only 48% of full chains (0.9^7). Chained evaluation reveals failure patterns that parallel benchmarks miss. Dual-Modality Testing S5 runs in two modes to measure how much model performance depends on context availability: Mode Input Purpose S5:cb Case metadata + S4 extracted holding Reasoning from minimal context S5:rag Full opinion texts for both cases Reasoning with complete information The difference in accuracy between modes\u2014the Reasoning Bridge Gap \u2014quantifies the contribution of retrieval versus reasoning from extracted information alone. Both variants use the same ground truth ( edge.agree ), enabling direct comparison. Run a Chain from chain.datasets.loaders import load_datasets from chain.datasets.builder import DatasetBuilder from chain.backends.mock_backend import MockBackend from chain.runner.executor import ChainExecutor from chain.steps import ( S1KnownAuthority , S2UnknownAuthority , S3ValidateAuthority , S4FactExtraction , S5DistinguishCB , S5DistinguishRAG , S6IRACSynthesis , S7CitationIntegrity ) # Load data bundle = load_datasets () builder = DatasetBuilder ( bundle ) # Get one instance instance = next ( builder . iter_chain_instances ()) # Build chain steps = [ S1KnownAuthority (), S2UnknownAuthority (), S3ValidateAuthority (), S4FactExtraction (), S5DistinguishCB (), S5DistinguishRAG (), S6IRACSynthesis (), S7CitationIntegrity (), ] # Execute backend = MockBackend () # Or real LLM backend executor = ChainExecutor ( backend = backend , steps = steps ) result = executor . execute ( instance ) # Results for step_id , sr in result . step_results . items (): print ( f \" { step_id } : score= { sr . score : .2f } , correct= { sr . correct } \" ) Data Source This project uses data from the Dahl et al. Legal Hallucinations Dataset. (https://huggingface.co/datasets/reglab/legal_hallucinations_paper_data): Source Size Contents scdb_sample.csv 5,000 cases SCOTUS cases with full opinion text scotus_shepards_sample.csv 5,000 pairs Case citation relationships scotus_overruled_db.csv 288 records Overruling relationships fake_cases.csv 999 cases Fabricated cases for hallucination detection Coverage Tiers The two sample files are independently drawn, creating coverage tiers: Tier Requirement Available Steps CHAIN_CORE Cited case has opinion text S1, S2, S3, S4, S5:cb, S6, S7 CHAIN_RAG_SUBSET Both cases have opinion text All steps including S5:rag Metrics Per-Step Metric Definition Accuracy correct / executed Mean Score mean(score) for executed steps Coverage executed / total instances Chain-Level Metric Definition Chain Completion Rate Chains with all steps correct / total Mean Failure Position Average step index of first error (1-indexed) Void Rate Chains voided by S7 gate / total S5 Comparison Metric Definition Reasoning Bridge Gap S5:rag accuracy \u2212 S5:cb accuracy (aligned subset) Architecture legal - 10 / \u251c\u2500\u2500 core / # Frozen contracts \u2502 \u251c\u2500\u2500 ids / # Canonical ID generation \u2502 \u251c\u2500\u2500 schemas / # CourtCase, ChainInstance, StepResult \u2502 \u2514\u2500\u2500 scoring / # Deterministic scorers \u2502 \u251c\u2500\u2500 chain / # Execution engine \u2502 \u251c\u2500\u2500 datasets / # HuggingFace loaders, instance builder \u2502 \u251c\u2500\u2500 backends / # LLM backends (Mock, Dahl, etc.) \u2502 \u251c\u2500\u2500 steps / # S1-S7 implementations \u2502 \u2514\u2500\u2500 runner / # ChainExecutor state machine \u2502 \u251c\u2500\u2500 scripts / # CLI tools \u251c\u2500\u2500 tests / # 247 tests \u2514\u2500\u2500 legal - 10 - notes / # Specifications Citation @article { chung2025l10 , title = {L10 Agentic: A Chained Evaluation Protocol for Legal Reasoning in Large Language Models} , author = {Chung, Jon W.} , journal = {scheduled; info will be released shortly} year ={2025 } } Original Data Source This project uses data from the Legal Hallucinations study. Please also cite: @article { dahl2024largelegalfictions , title = {Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models} , author = {Matthew Dahl and Varun Magesh and Mirac Suzgun and Daniel E. Ho} , year = {2024} , journal = {Journal of Legal Analysis} , volume = {16} , number = {1} , pages = {64--93} , doi = {10.1093/jla/laae003} } License MIT Roadmap v1.0 (Current) \u2014 MVP Complete \u2713 [x] 7-step chain architecture [x] All steps implemented (S1-S7) [x] ChainExecutor with dependency resolution [x] S5 dual-modality (CB + RAG) [x] S7 citation integrity gate [x] MockBackend for testing [x] 247 tests passing [x] HuggingFace data integration v1.1 \u2014 Production Runs [ ] Real LLM backend integration [ ] Pilot run (100 instances) [ ] Full dataset run (CHAIN_CORE) [ ] JSONL + Markdown output [ ] Results visualization [ ] Reasoning Bridge Gap measurement v2.0 \u2014 External Benchmark Integration HELM Integration L10 Chain HELM Scenarios \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 S3 Validate Authority \u2190\u2500\u2500\u2500\u2192 LegalBench overruling S4 Fact Extraction \u2190\u2500\u2500\u2500\u2192 LegalBench definition extraction S5:cb Distinguish \u2190\u2500\u2500\u2500\u2192 CaseHOLD 5-way MCQ Additional Benchmarks Benchmark Jurisdiction Integration Point CaseHOLD US S5:cb comparison baseline LegalBench US S3, S4 task injection LegalAgentBench Chinese Tool-calling evaluation Multi-Judge S6 class S6MultiJudge ( Step ): judges = [ \"gpt-4\" , \"claude-3\" , \"gemini\" ] def score ( self , parsed , ctx ): scores = [ judge . evaluate ( parsed , RUBRIC ) for judge in self . judges ] return mean ( scores ), std ( scores ) v2.1 \u2014 Agentic Mode Model-controlled retrieval with tool-calling: class AgenticStep ( Step ): \"\"\"Model decides what to retrieve\"\"\" tools : list [ Tool ] max_turns : int = 5 def run ( self , ctx , backend ): while not done and turns < max_turns : action = backend . decide ( ctx , self . tools ) observation = action . execute () ctx . update ( observation ) return self . synthesize ( ctx ) Contributing Fork the repository Create a feature branch Ensure all tests pass ( pytest tests/ -v ) Submit a pull request See legal-10-notes/L10_AGENTIC_SPEC.md for detailed specifications. Contact Questions? Open an issue or contact jondev717@gmail.com . Measuring what matters: not just legal knowledge, but legal reasoning.","title":"Home"},{"location":"#a-chained-legal-reasoning-benchmark-for-large-language-models","text":"","title":"A Chained Legal Reasoning Benchmark for Large Language Models"},{"location":"#the-problem","text":"Existing legal AI benchmarks evaluate models on independent, parallel tasks . A model answers a citation question, gets scored, then answers an unrelated extraction question, gets scored again. Each task exists in isolation. But legal reasoning doesn't work that way. A lawyer researching case law performs sequential, dependent operations : find relevant authority \u2192 verify it's still good law \u2192 extract the holding \u2192 apply it to new facts \u2192 synthesize into analysis. Each step builds on the last. An error at step 2 corrupts everything downstream. L10 Agentic is the first U.S. common law benchmark that evaluates LLMs on chained legal reasoning \u2014measuring not just whether models get individual answers right, but whether they can maintain coherent reasoning across a complete legal research workflow.","title":"The Problem"},{"location":"#the-chain","text":"RULE PHASE S1 Known Authority S2 Unknown Authority S3 Validate Authority \u2193 APPLICATION PHASE S4 Fact Extraction S5 Distinguish Cases ( cb + rag variants ) \u2193 CONCLUSION PHASE S6 IRAC Synthesis S7 Citation Integrity \u2190 gate : fabrication voids S6","title":"The Chain"},{"location":"#step-definitions","text":"Step Name Task Ground Truth S1 Known Authority Retrieve citation and metadata for a given case cited_case.us_cite , term S2 Unknown Authority Find cases that cite the anchor case edge.citing_case_us_cite (MRR) S3 Validate Authority Determine if case has been overruled instance.overrule S4 Fact Extraction Extract disposition and winning party SCDB codes (closed enum) S5:cb Distinguish Cases Determine doctrinal relationship (metadata + holding) edge.agree S5:rag Distinguish Cases Determine doctrinal relationship (full opinion texts) edge.agree S6 IRAC Synthesis Generate structured legal analysis Deterministic rubric S7 Citation Integrity Verify no fabricated citations fake_cases + scdb A model achieving 90% accuracy per step completes only 48% of full chains (0.9^7). Chained evaluation reveals failure patterns that parallel benchmarks miss.","title":"Step Definitions"},{"location":"#dual-modality-testing","text":"S5 runs in two modes to measure how much model performance depends on context availability: Mode Input Purpose S5:cb Case metadata + S4 extracted holding Reasoning from minimal context S5:rag Full opinion texts for both cases Reasoning with complete information The difference in accuracy between modes\u2014the Reasoning Bridge Gap \u2014quantifies the contribution of retrieval versus reasoning from extracted information alone. Both variants use the same ground truth ( edge.agree ), enabling direct comparison.","title":"Dual-Modality Testing"},{"location":"#run-a-chain","text":"from chain.datasets.loaders import load_datasets from chain.datasets.builder import DatasetBuilder from chain.backends.mock_backend import MockBackend from chain.runner.executor import ChainExecutor from chain.steps import ( S1KnownAuthority , S2UnknownAuthority , S3ValidateAuthority , S4FactExtraction , S5DistinguishCB , S5DistinguishRAG , S6IRACSynthesis , S7CitationIntegrity ) # Load data bundle = load_datasets () builder = DatasetBuilder ( bundle ) # Get one instance instance = next ( builder . iter_chain_instances ()) # Build chain steps = [ S1KnownAuthority (), S2UnknownAuthority (), S3ValidateAuthority (), S4FactExtraction (), S5DistinguishCB (), S5DistinguishRAG (), S6IRACSynthesis (), S7CitationIntegrity (), ] # Execute backend = MockBackend () # Or real LLM backend executor = ChainExecutor ( backend = backend , steps = steps ) result = executor . execute ( instance ) # Results for step_id , sr in result . step_results . items (): print ( f \" { step_id } : score= { sr . score : .2f } , correct= { sr . correct } \" )","title":"Run a Chain"},{"location":"#data-source","text":"This project uses data from the Dahl et al. Legal Hallucinations Dataset. (https://huggingface.co/datasets/reglab/legal_hallucinations_paper_data): Source Size Contents scdb_sample.csv 5,000 cases SCOTUS cases with full opinion text scotus_shepards_sample.csv 5,000 pairs Case citation relationships scotus_overruled_db.csv 288 records Overruling relationships fake_cases.csv 999 cases Fabricated cases for hallucination detection","title":"Data Source"},{"location":"#coverage-tiers","text":"The two sample files are independently drawn, creating coverage tiers: Tier Requirement Available Steps CHAIN_CORE Cited case has opinion text S1, S2, S3, S4, S5:cb, S6, S7 CHAIN_RAG_SUBSET Both cases have opinion text All steps including S5:rag","title":"Coverage Tiers"},{"location":"#metrics","text":"","title":"Metrics"},{"location":"#per-step","text":"Metric Definition Accuracy correct / executed Mean Score mean(score) for executed steps Coverage executed / total instances","title":"Per-Step"},{"location":"#chain-level","text":"Metric Definition Chain Completion Rate Chains with all steps correct / total Mean Failure Position Average step index of first error (1-indexed) Void Rate Chains voided by S7 gate / total","title":"Chain-Level"},{"location":"#s5-comparison","text":"Metric Definition Reasoning Bridge Gap S5:rag accuracy \u2212 S5:cb accuracy (aligned subset)","title":"S5 Comparison"},{"location":"#architecture","text":"legal - 10 / \u251c\u2500\u2500 core / # Frozen contracts \u2502 \u251c\u2500\u2500 ids / # Canonical ID generation \u2502 \u251c\u2500\u2500 schemas / # CourtCase, ChainInstance, StepResult \u2502 \u2514\u2500\u2500 scoring / # Deterministic scorers \u2502 \u251c\u2500\u2500 chain / # Execution engine \u2502 \u251c\u2500\u2500 datasets / # HuggingFace loaders, instance builder \u2502 \u251c\u2500\u2500 backends / # LLM backends (Mock, Dahl, etc.) \u2502 \u251c\u2500\u2500 steps / # S1-S7 implementations \u2502 \u2514\u2500\u2500 runner / # ChainExecutor state machine \u2502 \u251c\u2500\u2500 scripts / # CLI tools \u251c\u2500\u2500 tests / # 247 tests \u2514\u2500\u2500 legal - 10 - notes / # Specifications","title":"Architecture"},{"location":"#citation","text":"@article { chung2025l10 , title = {L10 Agentic: A Chained Evaluation Protocol for Legal Reasoning in Large Language Models} , author = {Chung, Jon W.} , journal = {scheduled; info will be released shortly} year ={2025 } }","title":"Citation"},{"location":"#original-data-source","text":"This project uses data from the Legal Hallucinations study. Please also cite: @article { dahl2024largelegalfictions , title = {Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models} , author = {Matthew Dahl and Varun Magesh and Mirac Suzgun and Daniel E. Ho} , year = {2024} , journal = {Journal of Legal Analysis} , volume = {16} , number = {1} , pages = {64--93} , doi = {10.1093/jla/laae003} }","title":"Original Data Source"},{"location":"#license","text":"MIT","title":"License"},{"location":"#roadmap","text":"","title":"Roadmap"},{"location":"#v10-current-mvp-complete","text":"[x] 7-step chain architecture [x] All steps implemented (S1-S7) [x] ChainExecutor with dependency resolution [x] S5 dual-modality (CB + RAG) [x] S7 citation integrity gate [x] MockBackend for testing [x] 247 tests passing [x] HuggingFace data integration","title":"v1.0 (Current) \u2014 MVP Complete \u2713"},{"location":"#v11-production-runs","text":"[ ] Real LLM backend integration [ ] Pilot run (100 instances) [ ] Full dataset run (CHAIN_CORE) [ ] JSONL + Markdown output [ ] Results visualization [ ] Reasoning Bridge Gap measurement","title":"v1.1 \u2014 Production Runs"},{"location":"#v20-external-benchmark-integration","text":"","title":"v2.0 \u2014 External Benchmark Integration"},{"location":"#helm-integration","text":"L10 Chain HELM Scenarios \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 S3 Validate Authority \u2190\u2500\u2500\u2500\u2192 LegalBench overruling S4 Fact Extraction \u2190\u2500\u2500\u2500\u2192 LegalBench definition extraction S5:cb Distinguish \u2190\u2500\u2500\u2500\u2192 CaseHOLD 5-way MCQ","title":"HELM Integration"},{"location":"#additional-benchmarks","text":"Benchmark Jurisdiction Integration Point CaseHOLD US S5:cb comparison baseline LegalBench US S3, S4 task injection LegalAgentBench Chinese Tool-calling evaluation","title":"Additional Benchmarks"},{"location":"#multi-judge-s6","text":"class S6MultiJudge ( Step ): judges = [ \"gpt-4\" , \"claude-3\" , \"gemini\" ] def score ( self , parsed , ctx ): scores = [ judge . evaluate ( parsed , RUBRIC ) for judge in self . judges ] return mean ( scores ), std ( scores )","title":"Multi-Judge S6"},{"location":"#v21-agentic-mode","text":"Model-controlled retrieval with tool-calling: class AgenticStep ( Step ): \"\"\"Model decides what to retrieve\"\"\" tools : list [ Tool ] max_turns : int = 5 def run ( self , ctx , backend ): while not done and turns < max_turns : action = backend . decide ( ctx , self . tools ) observation = action . execute () ctx . update ( observation ) return self . synthesize ( ctx )","title":"v2.1 \u2014 Agentic Mode"},{"location":"#contributing","text":"Fork the repository Create a feature branch Ensure all tests pass ( pytest tests/ -v ) Submit a pull request See legal-10-notes/L10_AGENTIC_SPEC.md for detailed specifications.","title":"Contributing"},{"location":"#contact","text":"Questions? Open an issue or contact jondev717@gmail.com . Measuring what matters: not just legal knowledge, but legal reasoning.","title":"Contact"},{"location":"api/","text":"Detailed API documentation for Legal-10. ChainExecutor The main entry point for running evaluations. Constructor ChainExecutor ( backend , steps = None ) Parameters: backend - The LLM backend to use steps - Optional list of steps (uses defaults if not provided) Methods execute def execute ( self , instance : ChainInstance ) -> ChainResult Runs all steps on a single instance. Parameters: instance - The chain instance to evaluate Returns: ChainResult with step-by-step scores Steps All steps inherit from the base Step class. S1KnownAuthority Retrieves citation and metadata for a given case. Input Output Ground Truth Case name Citation cited_case.us_cite S2UnknownAuthority Finds cases that cite the anchor case. S3ValidateAuthority Determines if a case has been overruled. S4FactExtraction Extracts disposition and winning party. S5DistinguishCB Determines doctrinal relationship using metadata. S5DistinguishRAG Determines doctrinal relationship using full text. S6IRACSynthesis Generates structured legal analysis. S7CitationIntegrity Verifies no fabricated citations exist. Backends MockBackend For testing without API calls. backend = MockBackend () Creating Custom Backends Implement the Backend protocol: class MyBackend : def complete ( self , prompt : str ) -> str : # Your implementation pass","title":"Api"},{"location":"api/#chainexecutor","text":"The main entry point for running evaluations.","title":"ChainExecutor"},{"location":"api/#constructor","text":"ChainExecutor ( backend , steps = None ) Parameters: backend - The LLM backend to use steps - Optional list of steps (uses defaults if not provided)","title":"Constructor"},{"location":"api/#methods","text":"","title":"Methods"},{"location":"api/#execute","text":"def execute ( self , instance : ChainInstance ) -> ChainResult Runs all steps on a single instance. Parameters: instance - The chain instance to evaluate Returns: ChainResult with step-by-step scores","title":"execute"},{"location":"api/#steps","text":"All steps inherit from the base Step class.","title":"Steps"},{"location":"api/#s1knownauthority","text":"Retrieves citation and metadata for a given case. Input Output Ground Truth Case name Citation cited_case.us_cite","title":"S1KnownAuthority"},{"location":"api/#s2unknownauthority","text":"Finds cases that cite the anchor case.","title":"S2UnknownAuthority"},{"location":"api/#s3validateauthority","text":"Determines if a case has been overruled.","title":"S3ValidateAuthority"},{"location":"api/#s4factextraction","text":"Extracts disposition and winning party.","title":"S4FactExtraction"},{"location":"api/#s5distinguishcb","text":"Determines doctrinal relationship using metadata.","title":"S5DistinguishCB"},{"location":"api/#s5distinguishrag","text":"Determines doctrinal relationship using full text.","title":"S5DistinguishRAG"},{"location":"api/#s6iracsynthesis","text":"Generates structured legal analysis.","title":"S6IRACSynthesis"},{"location":"api/#s7citationintegrity","text":"Verifies no fabricated citations exist.","title":"S7CitationIntegrity"},{"location":"api/#backends","text":"","title":"Backends"},{"location":"api/#mockbackend","text":"For testing without API calls. backend = MockBackend ()","title":"MockBackend"},{"location":"api/#creating-custom-backends","text":"Implement the Backend protocol: class MyBackend : def complete ( self , prompt : str ) -> str : # Your implementation pass","title":"Creating Custom Backends"},{"location":"architecture/","text":"Overview of the Legal-10 codebase structure. Directory Layout legal - 10 / \u251c\u2500\u2500 core / # Frozen contracts \u251c\u2500\u2500 chain / # Execution engine \u251c\u2500\u2500 scripts / # CLI tools \u251c\u2500\u2500 tests / # 247 tests \u2514\u2500\u2500 legal - 10 - notes / # Specifications Core Module Contains the foundational schemas and contracts. Schemas CourtCase - Represents a legal case ChainInstance - A single evaluation instance StepResult - Output from each step Scoring Deterministic scoring logic for each step type. Chain Module The execution engine that runs evaluations. Steps Step Name Purpose S1 Known Authority Retrieve citation S2 Unknown Authority Find citing cases S3 Validate Authority Check overruling S4 Fact Extraction Extract disposition S5 Distinguish Cases Doctrinal relationship S6 IRAC Synthesis Generate analysis S7 Citation Integrity Verify citations Backends MockBackend - For testing DahlBackend - Real LLM integration Data Flow Load datasets from HuggingFace Build chain instances Execute through steps Score and aggregate results","title":"Architecture"},{"location":"architecture/#directory-layout","text":"legal - 10 / \u251c\u2500\u2500 core / # Frozen contracts \u251c\u2500\u2500 chain / # Execution engine \u251c\u2500\u2500 scripts / # CLI tools \u251c\u2500\u2500 tests / # 247 tests \u2514\u2500\u2500 legal - 10 - notes / # Specifications","title":"Directory Layout"},{"location":"architecture/#core-module","text":"Contains the foundational schemas and contracts.","title":"Core Module"},{"location":"architecture/#schemas","text":"CourtCase - Represents a legal case ChainInstance - A single evaluation instance StepResult - Output from each step","title":"Schemas"},{"location":"architecture/#scoring","text":"Deterministic scoring logic for each step type.","title":"Scoring"},{"location":"architecture/#chain-module","text":"The execution engine that runs evaluations.","title":"Chain Module"},{"location":"architecture/#steps","text":"Step Name Purpose S1 Known Authority Retrieve citation S2 Unknown Authority Find citing cases S3 Validate Authority Check overruling S4 Fact Extraction Extract disposition S5 Distinguish Cases Doctrinal relationship S6 IRAC Synthesis Generate analysis S7 Citation Integrity Verify citations","title":"Steps"},{"location":"architecture/#backends","text":"MockBackend - For testing DahlBackend - Real LLM integration","title":"Backends"},{"location":"architecture/#data-flow","text":"Load datasets from HuggingFace Build chain instances Execute through steps Score and aggregate results","title":"Data Flow"},{"location":"getting_started/","text":"How to install and run Legal-10. Run Your First Chain H","title":"Getting started"},{"location":"getting_started/#run-your-first-chain","text":"H","title":"Run Your First Chain"},{"location":"roadmap/","text":"Future development plans for Legal-10. v1.0 (Current) MVP Complete. [x] 7-step chain architecture [x] All steps implemented (S1-S7) [x] ChainExecutor with dependency resolution [x] S5 dual-modality (CB + RAG) [x] S7 citation integrity gate [x] MockBackend for testing [x] 247 tests passing [x] HuggingFace data integration v1.1 \u2014 Production Runs [ ] Real LLM backend integration [ ] Pilot run (100 instances) [ ] Full dataset run (CHAIN_CORE) [ ] JSONL + Markdown output [ ] Results visualization [ ] Reasoning Bridge Gap measurement v2.0 \u2014 External Benchmark Integration HELM Integration L10 Step HELM Scenario S3 Validate Authority LegalBench overruling S4 Fact Extraction LegalBench definition extraction S5:cb Distinguish CaseHOLD 5-way MCQ Additional Benchmarks Benchmark Jurisdiction Integration Point CaseHOLD US S5:cb comparison baseline LegalBench US S3, S4 task injection LegalAgentBench Chinese Tool-calling evaluation v2.1 \u2014 Agentic Mode Model-controlled retrieval with tool-calling capabilities. class AgenticStep ( Step ): tools : list [ Tool ] max_turns : int = 5 Contributing Fork the repository Create a feature branch Ensure all tests pass Submit a pull request","title":"Roadmap"},{"location":"roadmap/#v10-current","text":"MVP Complete. [x] 7-step chain architecture [x] All steps implemented (S1-S7) [x] ChainExecutor with dependency resolution [x] S5 dual-modality (CB + RAG) [x] S7 citation integrity gate [x] MockBackend for testing [x] 247 tests passing [x] HuggingFace data integration","title":"v1.0 (Current)"},{"location":"roadmap/#v11-production-runs","text":"[ ] Real LLM backend integration [ ] Pilot run (100 instances) [ ] Full dataset run (CHAIN_CORE) [ ] JSONL + Markdown output [ ] Results visualization [ ] Reasoning Bridge Gap measurement","title":"v1.1 \u2014 Production Runs"},{"location":"roadmap/#v20-external-benchmark-integration","text":"","title":"v2.0 \u2014 External Benchmark Integration"},{"location":"roadmap/#helm-integration","text":"L10 Step HELM Scenario S3 Validate Authority LegalBench overruling S4 Fact Extraction LegalBench definition extraction S5:cb Distinguish CaseHOLD 5-way MCQ","title":"HELM Integration"},{"location":"roadmap/#additional-benchmarks","text":"Benchmark Jurisdiction Integration Point CaseHOLD US S5:cb comparison baseline LegalBench US S3, S4 task injection LegalAgentBench Chinese Tool-calling evaluation","title":"Additional Benchmarks"},{"location":"roadmap/#v21-agentic-mode","text":"Model-controlled retrieval with tool-calling capabilities. class AgenticStep ( Step ): tools : list [ Tool ] max_turns : int = 5","title":"v2.1 \u2014 Agentic Mode"},{"location":"roadmap/#contributing","text":"Fork the repository Create a feature branch Ensure all tests pass Submit a pull request","title":"Contributing"}]}